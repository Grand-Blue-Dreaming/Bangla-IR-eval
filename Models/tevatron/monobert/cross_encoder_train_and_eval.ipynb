{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sI4zmBV31hD"
   },
   "source": [
    "## Retrieval using BM25 (pyserini implementation)\n",
    "Reranking on the whole dataset is very costly. So, as per usual practice, an initial list (of 1000 candidates) is generated using a fast retriever such as BM25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OnErIWIcqWTM",
    "outputId": "6d5e430a-420b-454b-faf2-bd903bf548f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "2024-01-28 08:00:24,322 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:204) - Setting log level to INFO\n",
      "2024-01-28 08:00:24,324 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:207) - ============ Loading Index Configuration ============\n",
      "2024-01-28 08:00:24,324 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:208) - AbstractIndexer settings:\n",
      "2024-01-28 08:00:24,325 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:209) -  + DocumentCollection path: /content/gdrive/MyDrive/Research/predefence/anwesha/anwesha-travel-collection/\n",
      "2024-01-28 08:00:24,325 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:210) -  + CollectionClass: JsonCollection\n",
      "2024-01-28 08:00:24,326 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:211) -  + Index path: indexes/anwesha-travel-bm25\n",
      "2024-01-28 08:00:24,326 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:212) -  + Threads: 1\n",
      "2024-01-28 08:00:24,327 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:213) -  + Optimize (merge segments)? false\n",
      "2024-01-28 08:00:24,390 INFO  [main] index.IndexCollection (IndexCollection.java:237) - Using language-specific analyzer\n",
      "2024-01-28 08:00:24,390 INFO  [main] index.IndexCollection (IndexCollection.java:238) - Language: bn\n",
      "2024-01-28 08:00:24,676 INFO  [main] index.IndexCollection (IndexCollection.java:197) - IndexCollection settings:\n",
      "2024-01-28 08:00:24,676 INFO  [main] index.IndexCollection (IndexCollection.java:198) -  + Generator: DefaultLuceneDocumentGenerator\n",
      "2024-01-28 08:00:24,677 INFO  [main] index.IndexCollection (IndexCollection.java:199) -  + Language: bn\n",
      "2024-01-28 08:00:24,677 INFO  [main] index.IndexCollection (IndexCollection.java:200) -  + Stemmer: porter\n",
      "2024-01-28 08:00:24,677 INFO  [main] index.IndexCollection (IndexCollection.java:201) -  + Keep stopwords? false\n",
      "2024-01-28 08:00:24,678 INFO  [main] index.IndexCollection (IndexCollection.java:202) -  + Stopwords: null\n",
      "2024-01-28 08:00:24,678 INFO  [main] index.IndexCollection (IndexCollection.java:203) -  + Store positions? true\n",
      "2024-01-28 08:00:24,678 INFO  [main] index.IndexCollection (IndexCollection.java:204) -  + Store docvectors? true\n",
      "2024-01-28 08:00:24,678 INFO  [main] index.IndexCollection (IndexCollection.java:205) -  + Store document \"contents\" field? false\n",
      "2024-01-28 08:00:24,679 INFO  [main] index.IndexCollection (IndexCollection.java:206) -  + Store document \"raw\" field? true\n",
      "2024-01-28 08:00:24,679 INFO  [main] index.IndexCollection (IndexCollection.java:207) -  + Additional fields to index: []\n",
      "2024-01-28 08:00:24,679 INFO  [main] index.IndexCollection (IndexCollection.java:208) -  + Whitelist: null\n",
      "2024-01-28 08:00:24,679 INFO  [main] index.IndexCollection (IndexCollection.java:209) -  + Pretokenized?: false\n",
      "2024-01-28 08:00:24,680 INFO  [main] index.IndexCollection (IndexCollection.java:210) -  + Codec: Lucene99\n",
      "2024-01-28 08:00:24,680 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:237) - ============ Indexing Collection ============\n",
      "2024-01-28 08:00:24,691 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:246) - Thread pool with 1 threads initialized.\n",
      "2024-01-28 08:00:24,692 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:247) - 1 file found in /content/gdrive/MyDrive/Research/predefence/anwesha/anwesha-travel-collection \n",
      "2024-01-28 08:00:24,692 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:248) - Starting to index...\n",
      "2024-01-28 08:00:27,670 DEBUG [pool-2-thread-1] index.AbstractIndexer$IndexerThread (AbstractIndexer.java:175) - anwesha-travel-collection/anwesha-travel-collection.jsonl: 214 docs added.\n",
      "2024-01-28 08:00:28,522 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:306) - Indexing Complete! 214 documents indexed\n",
      "2024-01-28 08:00:28,522 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:307) - ============ Final Counter Values ============\n",
      "2024-01-28 08:00:28,522 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:308) - indexed:              214\n",
      "2024-01-28 08:00:28,522 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:309) - unindexable:            0\n",
      "2024-01-28 08:00:28,524 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:310) - empty:                  0\n",
      "2024-01-28 08:00:28,525 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:311) - skipped:                0\n",
      "2024-01-28 08:00:28,525 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:312) - errors:                 0\n",
      "2024-01-28 08:00:28,540 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:315) - Total 214 documents indexed in 00:00:03\n",
      "Running /content/gdrive/MyDrive/Research/predefence/anwesha/anwesha-travel-topics.tsv topics, saving to run.anwesha-travel.bm25.txt...\n",
      "100% 40/40 [00:02<00:00, 14.47it/s]\n",
      "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
      "/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
      "Skipping download.\n",
      "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-M', '100', '-m', 'ndcg_cut.10', '-m', 'recall.100', '/content/gdrive/MyDrive/Research/predefence/anwesha/anwesha-travel-qrels.txt', 'run.anwesha-travel.bm25.txt']\n",
      "Results:\n",
      "recall_100            \tall\t0.9853\n",
      "ndcg_cut_10           \tall\t0.7322\n"
     ]
    }
   ],
   "source": [
    "data_folder_root = \"/media/cse/HDD/SamiKhan/anwesha\"\n",
    "dataset_name = \"anwesha-travel\"\n",
    "topics_path = f\"{data_folder_root}/{dataset_name}-topics.tsv\"\n",
    "qrels_path = f\"{data_folder_root}/{dataset_name}-qrels.txt\"\n",
    "\n",
    "!python -m pyserini.index.lucene \\\n",
    "  --collection JsonCollection \\\n",
    "  --input {data_folder_root}/{dataset_name}-collection/ \\\n",
    "  --language bn \\\n",
    "  --index indexes/{dataset_name}-bm25 \\\n",
    "  --generator DefaultLuceneDocumentGenerator \\\n",
    "  --threads 1 \\\n",
    "  --storePositions --storeDocvectors --storeRaw\n",
    "\n",
    "!python -m pyserini.search.lucene \\\n",
    "  --index indexes/{dataset_name}-bm25 \\\n",
    "  --topics {topics_path} \\\n",
    "  --output run.{dataset_name}.bm25.txt \\\n",
    "  --language bn \\\n",
    "  --bm25\n",
    "\n",
    "!python -m pyserini.eval.trec_eval \\\n",
    "  -c -M 100 -m ndcg_cut.10 -m recall.100 {qrels_path} \\\n",
    "  run.{dataset_name}.bm25.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /home/cse/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
      "/home/cse/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
      "Skipping download.\n",
      "Running command: ['java', '-jar', '/home/cse/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-M', '100', '-m', 'ndcg_cut.10', '-m', 'recall.100', '/media/cse/HDD/SamiKhan/anwesha/anwesha-travel-qrels.txt', '/media/cse/HDD/SamiKhan/anwesha/runs/run.anwesha-travel.bm25.txt']\n",
      "Results:\n",
      "recall_100            \tall\t0.9853\n",
      "ndcg_cut_10           \tall\t0.7322\n"
     ]
    }
   ],
   "source": [
    "data_folder_root = \"/media/cse/HDD/SamiKhan/anwesha\"\n",
    "dataset_name = \"anwesha-travel\"\n",
    "qrels_path = f\"{data_folder_root}/{dataset_name}-qrels.txt\"\n",
    "run_path = f\"{data_folder_root}/runs/run.{dataset_name}.bm25.txt\"\n",
    "\n",
    "!python -m pyserini.eval.trec_eval \\\n",
    "  -c -M 100 -m ndcg_cut.10 -m recall.100 {qrels_path} \\\n",
    "  {run_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXB5NfOW5zgR"
   },
   "source": [
    "## Training Reranker on Mr. TyDi Bangla Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "guaSJK1a3vmV",
    "outputId": "360ca2b0-8b3a-4128-9537-a61b1f3c40ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/cse/.cache/huggingface/token\n",
      "Login successful\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "01/30/2024 17:15:59 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: True\n",
      "01/30/2024 17:15:59 - INFO - __main__ -   Training/evaluation parameters TevatronTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_encode=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gc_p_chunk_size=32,\n",
      "gc_q_chunk_size=4,\n",
      "grad_cache=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-06,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=reranker_mrtydi-bn/runs/Jan30_17-15-59_DeepLearning2,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "negatives_x_device=False,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=reranker_mrtydi-bn,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=reranker_mrtydi-bn,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.1,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "01/30/2024 17:15:59 - INFO - __main__ -   MODEL parameters ModelArguments(model_name_or_path='bert-base-multilingual-uncased', config_name=None, tokenizer_name=None, cache_dir=None, untie_encoder=False, add_pooler=False, projection_in_dim=768, projection_out_dim=768, normalize=False, dtype='float32')\n",
      "01/30/2024 17:16:06 - INFO - tevatron.reranker.modeling -   loading model weight from huggingface bert-base-multilingual-uncased\n",
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "01/30/2024 17:16:11 - WARNING - datasets.builder -   Reusing dataset mr_ty_di (/home/cse/.cache/huggingface/datasets/castorini___mr_ty_di/bengali/1.1.0/b1e30fc714b84f518e8254cb14949053c750761bed4a24764dabc377d2ca5678)\n",
      "100%|███████████████████████████████████████████| 3/3 [00:00<00:00, 1781.02it/s]\n",
      "01/30/2024 17:16:11 - WARNING - datasets.fingerprint -   Parameter 'function'=<function Dataset.map.<locals>.decorate.<locals>.decorated at 0x7f5948ca19e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Running tokenizer on train dataset #0:   0%|            | 0/143 [00:00<?, ?ex/s]\n",
      "Running tokenizer on train dataset #1:   0%|            | 0/143 [00:00<?, ?ex/s]\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:   0%|            | 0/143 [00:00<?, ?ex/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:   0%|            | 0/143 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:   0%|            | 0/143 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:   0%|            | 0/143 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:   0%|            | 0/143 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:   0%|            | 0/143 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:   5%|▏   | 7/143 [00:00<00:02, 60.08ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:   7%|▏  | 10/143 [00:00<00:01, 91.90ex/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:   0%|            | 0/142 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:   6%|▎   | 9/143 [00:00<00:01, 85.23ex/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:   0%|           | 0/142 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:   7%|▏  | 10/143 [00:00<00:01, 99.08ex/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:   0%|           | 0/142 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:   7%|▏  | 10/143 [00:00<00:01, 96.14ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:   5%|▏   | 7/143 [00:00<00:02, 61.79ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:   8%|▏ | 11/143 [00:00<00:01, 101.56ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:   7%|▏  | 10/143 [00:00<00:01, 90.09ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:   3%|▏   | 5/143 [00:00<00:03, 45.34ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  14%|▍  | 20/143 [00:00<00:01, 95.46ex/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:   6%|▎   | 9/142 [00:00<00:01, 89.09ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  13%|▍  | 19/143 [00:00<00:01, 90.42ex/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  10%|▎  | 14/143 [00:00<00:02, 56.16ex/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:   6%|▏  | 8/142 [00:00<00:01, 77.70ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:   4%|▏  | 6/142 [00:00<00:02, 50.56ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  15%|▎ | 21/143 [00:00<00:01, 100.62ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  15%|▍  | 22/143 [00:00<00:01, 99.10ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  10%|▎  | 14/143 [00:00<00:02, 58.29ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  14%|▍  | 20/143 [00:00<00:01, 92.06ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:   8%|▏  | 11/143 [00:00<00:02, 52.49ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  14%|▎ | 20/142 [00:00<00:01, 100.62ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  21%|▋  | 30/143 [00:00<00:01, 94.27ex/s]\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #0:  14%|▍  | 20/143 [00:00<00:02, 57.65ex/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  22%|▍ | 32/143 [00:00<00:01, 101.48ex/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  14%|▏| 20/142 [00:00<00:01, 101.73ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:   8%|▏ | 12/142 [00:00<00:02, 52.09ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  22%|▋  | 32/143 [00:00<00:01, 98.14ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  22%|▋  | 32/143 [00:00<00:01, 96.85ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  15%|▍  | 21/143 [00:00<00:02, 60.26ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  21%|▋  | 30/143 [00:00<00:01, 93.07ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  22%|▍ | 31/142 [00:00<00:01, 104.73ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  29%|▊  | 41/143 [00:00<00:01, 97.61ex/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  12%|▎  | 17/143 [00:00<00:02, 49.93ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  28%|▊  | 40/143 [00:00<00:01, 94.55ex/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  18%|▌  | 26/143 [00:00<00:02, 54.52ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  30%|▉  | 43/143 [00:00<00:01, 97.28ex/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  13%|▎ | 19/142 [00:00<00:02, 57.16ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  31%|▌ | 44/143 [00:00<00:00, 104.10ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  30%|▉  | 43/143 [00:00<00:01, 98.79ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  28%|▊  | 40/143 [00:00<00:01, 92.88ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  20%|▌  | 28/143 [00:00<00:01, 59.83ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  30%|▌ | 42/142 [00:00<00:00, 101.83ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  37%|▋ | 53/143 [00:00<00:00, 102.15ex/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  16%|▍  | 23/143 [00:00<00:02, 52.61ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  35%|█  | 50/143 [00:00<00:00, 94.51ex/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  30%|▎| 43/142 [00:00<00:00, 104.43ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  22%|▋  | 32/143 [00:00<00:02, 54.59ex/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  38%|▊ | 55/143 [00:00<00:00, 105.63ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  18%|▎ | 25/142 [00:00<00:02, 54.00ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  37%|█  | 53/143 [00:00<00:00, 97.54ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  36%|█  | 51/143 [00:00<00:00, 93.85ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  24%|▋  | 34/143 [00:00<00:01, 58.16ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  38%|▊ | 54/142 [00:00<00:00, 106.31ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  45%|▉ | 64/143 [00:00<00:00, 101.77ex/s]\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  42%|█▎ | 60/143 [00:00<00:00, 96.01ex/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  20%|▌  | 29/143 [00:00<00:02, 52.74ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  38%|▍| 54/142 [00:00<00:00, 101.13ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  45%|▉ | 65/143 [00:00<00:00, 101.12ex/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  27%|▊  | 38/143 [00:00<00:01, 53.67ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  23%|▍ | 32/142 [00:00<00:01, 56.39ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  44%|█▎ | 63/143 [00:00<00:00, 95.98ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  28%|▊  | 40/143 [00:00<00:01, 58.72ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  43%|█▎ | 62/143 [00:00<00:00, 96.70ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  46%|▉ | 66/142 [00:00<00:00, 109.75ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  52%|█ | 75/143 [00:00<00:00, 103.13ex/s]\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  50%|█ | 72/143 [00:00<00:00, 101.51ex/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  46%|▍| 66/142 [00:00<00:00, 105.77ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  24%|▋  | 35/143 [00:00<00:02, 51.64ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  54%|█ | 77/143 [00:00<00:00, 103.17ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  31%|▉  | 44/143 [00:00<00:01, 50.95ex/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  27%|▌ | 38/142 [00:00<00:01, 55.20ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  51%|█▌ | 73/143 [00:00<00:00, 95.53ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  32%|▉  | 46/143 [00:00<00:01, 58.59ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  50%|█▌ | 72/143 [00:00<00:00, 96.38ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  54%|█ | 77/142 [00:00<00:00, 107.40ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  60%|█▏| 86/143 [00:00<00:00, 101.92ex/s]\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  58%|█▋ | 83/143 [00:00<00:00, 98.95ex/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  54%|▌| 77/142 [00:00<00:00, 106.06ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  62%|█▏| 88/143 [00:00<00:00, 100.57ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  60%|█▊ | 86/143 [00:00<00:00, 95.39ex/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  35%|█  | 50/143 [00:00<00:01, 51.56ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  31%|▌ | 44/142 [00:00<00:01, 54.03ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  58%|█▋ | 83/143 [00:00<00:00, 91.80ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  36%|█  | 52/143 [00:00<00:01, 57.90ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  57%|█▋ | 82/143 [00:00<00:00, 95.68ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  62%|█▏| 88/142 [00:00<00:00, 105.04ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  68%|█▎| 97/143 [00:00<00:00, 101.70ex/s]\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  66%|█▎| 94/143 [00:00<00:00, 100.78ex/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  62%|▌| 88/142 [00:00<00:00, 105.34ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  67%|██ | 96/143 [00:00<00:00, 96.06ex/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  70%|▋| 100/143 [00:00<00:00, 104.50ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  34%|█  | 48/143 [00:00<00:01, 52.83ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  41%|█▏ | 58/143 [00:00<00:01, 57.20ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  64%|█▉ | 92/143 [00:00<00:00, 95.67ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  65%|█▉ | 93/143 [00:00<00:00, 90.11ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  39%|█▏ | 56/143 [00:01<00:01, 50.25ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  70%|█▍| 99/142 [00:00<00:00, 102.92ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  73%|▋| 105/143 [00:01<00:00, 102.10ex/s]\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  76%|█▌| 108/143 [00:01<00:00, 97.73ex/s]\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  75%|█▍| 107/143 [00:01<00:00, 98.66ex/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  70%|█▍| 99/142 [00:00<00:00, 97.69ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  78%|█▌| 111/143 [00:01<00:00, 99.67ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  38%|█▏ | 54/143 [00:01<00:01, 53.11ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  45%|█▎ | 64/143 [00:01<00:01, 57.73ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  71%|█▍| 102/143 [00:01<00:00, 95.62ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  43%|█▎ | 62/143 [00:01<00:01, 51.67ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  77%|▊| 110/142 [00:01<00:00, 103.63ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  39%|▊ | 56/142 [00:01<00:01, 52.09ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  81%|▊| 116/143 [00:01<00:00, 101.07ex/s]\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  83%|█▋| 118/143 [00:01<00:00, 96.61ex/s]\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  83%|█▋| 118/143 [00:01<00:00, 99.38ex/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  77%|▊| 109/142 [00:01<00:00, 95.53ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  85%|▊| 122/143 [00:01<00:00, 100.44ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  42%|█▎ | 60/143 [00:01<00:01, 54.37ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  50%|█▍ | 71/143 [00:01<00:01, 58.86ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  78%|█▌| 112/143 [00:01<00:00, 92.36ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  85%|▊| 121/142 [00:01<00:00, 102.37ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  44%|▊ | 62/142 [00:01<00:01, 53.13ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  48%|█▍ | 68/143 [00:01<00:01, 48.94ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  89%|▉| 127/143 [00:01<00:00, 101.00ex/s]\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  90%|█▊| 128/143 [00:01<00:00, 95.48ex/s]\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  90%|█▊| 128/143 [00:01<00:00, 99.53ex/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  84%|▊| 119/142 [00:01<00:00, 92.00ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  93%|▉| 133/143 [00:01<00:00, 100.63ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  46%|█▍ | 66/143 [00:01<00:01, 54.12ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  54%|█▌ | 77/143 [00:01<00:01, 57.69ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  85%|█▋| 122/143 [00:01<00:00, 91.68ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  93%|▉| 132/142 [00:01<00:00, 102.99ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  48%|▉ | 68/142 [00:01<00:01, 53.18ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  51%|█▌ | 73/143 [00:01<00:01, 49.20ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  97%|█▉| 138/143 [00:01<00:00, 96.53ex/s]\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  97%|█▉| 138/143 [00:01<00:00, 98.52ex/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  97%|█▉| 138/143 [00:01<00:00, 98.76ex/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4: 100%|█| 143/143 [00:01<00:00, 101.32ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1: 100%|██| 143/143 [00:01<00:00, 97.89ex/s]\n",
      "Running tokenizer on train dataset #2: 100%|██| 143/143 [00:01<00:00, 98.43ex/s]\n",
      "Running tokenizer on train dataset #3: 100%|██| 143/143 [00:01<00:00, 98.85ex/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  58%|█▋ | 83/143 [00:01<00:01, 57.56ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  50%|█▌ | 72/143 [00:01<00:01, 52.51ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9: 100%|█| 142/142 [00:01<00:00, 103.11ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  92%|█▊| 131/143 [00:01<00:00, 85.73ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  55%|█▋ | 79/143 [00:01<00:01, 50.24ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  98%|▉| 139/142 [00:01<00:00, 89.69ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  64%|█▉ | 92/143 [00:01<00:00, 66.59ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  56%|█▋ | 80/143 [00:01<00:01, 59.72ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7: 100%|██| 143/143 [00:01<00:00, 94.83ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #11: 100%|█| 142/142 [00:01<00:00, 97.10ex/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  99%|█▉| 141/143 [00:01<00:00, 88.55ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6: 100%|██| 143/143 [00:01<00:00, 91.83ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  62%|█▊ | 89/143 [00:01<00:00, 67.99ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  71%|█▍| 101/143 [00:01<00:00, 71.80ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  70%|█▍| 100/143 [00:01<00:00, 74.83ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  70%|█▍| 100/143 [00:01<00:00, 78.93ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  78%|█▌| 111/143 [00:01<00:00, 78.47ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  78%|█▌| 111/143 [00:01<00:00, 83.09ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  78%|█▌| 111/143 [00:01<00:00, 86.31ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  85%|█▋| 122/143 [00:01<00:00, 86.86ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  85%|█▋| 122/143 [00:01<00:00, 89.02ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  85%|█▋| 122/143 [00:01<00:00, 91.37ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  93%|█▊| 133/143 [00:01<00:00, 92.23ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5: 100%|██| 143/143 [00:02<00:00, 69.91ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  92%|█▊| 132/143 [00:02<00:00, 92.27ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0: 100%|██| 143/143 [00:02<00:00, 66.74ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #10: 100%|█| 142/142 [00:02<00:00, 69.70ex/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8: 100%|██| 143/143 [00:02<00:00, 67.86ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "01/30/2024 17:16:17 - WARNING - datasets.builder -   Using custom data configuration default-a095fc8a5193f7f3\n",
      "Downloading and preparing dataset json/default to /home/cse/.cache/huggingface/datasets/json/default-a095fc8a5193f7f3/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde...\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 13662.23it/s]\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 2468.69it/s]\n",
      "Dataset json downloaded and prepared to /home/cse/.cache/huggingface/datasets/json/default-a095fc8a5193f7f3/0.0.0/c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde. Subsequent calls will reuse this data.\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 93.29it/s]\n",
      "Running tokenizer on train dataset #0:   0%|           | 0/4259 [00:00<?, ?ex/s]\n",
      "Running tokenizer on train dataset #1:   0%|           | 0/4259 [00:00<?, ?ex/s]\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:   0%|           | 0/4258 [00:00<?, ?ex/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:   0%|           | 0/4258 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:   0%|           | 0/4258 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:   0%|           | 0/4258 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:   2%| | 87/4259 [00:00<00:04, 864.07ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:   2%| | 99/4259 [00:00<00:04, 986.95ex/s]\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:   2%| | 76/4258 [00:00<00:05, 759.58ex/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:   0%|           | 0/4258 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:   0%|           | 0/4258 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:   2%| | 71/4258 [00:00<00:05, 707.72ex/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:   2%| | 82/4258 [00:00<00:05, 807.34ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:   2%| | 67/4258 [00:00<00:06, 667.34ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:   0%|           | 0/4258 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:   2%| | 94/4258 [00:00<00:04, 934.75ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:   0%|          | 0/4258 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:   0%|          | 0/4258 [00:00<?, ?ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #0:   4%| | 174/4259 [00:00<00:05, 708.80ex/s\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:   1%| | 37/4258 [00:00<00:12, 347.52ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:   2%| | 68/4258 [00:00<00:06, 669.54ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:   4%| | 150/4258 [00:00<00:05, 743.33ex/s\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:   5%| | 196/4258 [00:00<00:03, 1028.52ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:   1%| | 42/4258 [00:00<00:10, 417.80ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:   5%| | 198/4259 [00:00<00:06, 657.26ex/s\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:   1%| | 44/4258 [00:00<00:09, 438.95ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:   4%| | 188/4258 [00:00<00:05, 803.09ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:   2%| | 76/4258 [00:00<00:05, 756.96ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:   6%| | 250/4258 [00:00<00:04, 830.32ex/s\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:   6%| | 273/4259 [00:00<00:04, 817.42ex/s\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:   2%| | 73/4258 [00:00<00:11, 348.97ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:   5%| | 225/4258 [00:00<00:05, 703.96ex/s\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:   7%| | 303/4258 [00:00<00:03, 1043.25ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:   2%| | 87/4258 [00:00<00:09, 432.04ex/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:   3%| | 135/4258 [00:00<00:08, 507.38ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:   7%| | 286/4258 [00:00<00:04, 875.27ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:   4%| | 158/4258 [00:00<00:05, 790.12ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:   8%| | 334/4258 [00:00<00:04, 828.09ex/s\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:   3%| | 108/4258 [00:00<00:11, 349.24ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:   6%| | 270/4259 [00:00<00:06, 578.48ex/s\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:   5%| | 224/4258 [00:00<00:07, 544.83ex/s\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:   7%| | 296/4258 [00:00<00:05, 667.71ex/s\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:   2%| | 88/4258 [00:00<00:13, 314.95ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:   5%| | 212/4258 [00:00<00:06, 608.12ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:   9%| | 401/4258 [00:00<00:03, 976.45ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  10%| | 408/4258 [00:00<00:04, 865.68ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:   3%| | 131/4258 [00:00<00:11, 368.60ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #0:   8%| | 357/4259 [00:00<00:06, 616.25ex/s\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:   3%| | 149/4258 [00:00<00:11, 369.90ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:   7%| | 281/4258 [00:00<00:07, 547.66ex/s\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:   9%| | 370/4258 [00:00<00:05, 684.82ex/s\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:   3%| | 137/4258 [00:00<00:10, 380.40ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:   6%| | 238/4258 [00:00<00:06, 624.17ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:   8%| | 332/4259 [00:00<00:07, 518.20ex/s\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:   7%| | 290/4258 [00:00<00:05, 667.10ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:   4%| | 182/4258 [00:00<00:09, 417.88ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  13%|▏| 534/4258 [00:00<00:03, 940.60ex/s\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  12%| | 500/4258 [00:00<00:04, 920.32ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  10%| | 425/4259 [00:00<00:06, 599.52ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:   8%| | 350/4258 [00:00<00:06, 592.32ex/s\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  11%| | 458/4258 [00:00<00:05, 746.21ex/s\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:   7%| | 318/4258 [00:00<00:05, 681.73ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:   4%| | 178/4258 [00:00<00:10, 382.09ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:   9%| | 391/4259 [00:00<00:07, 537.78ex/s\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:   9%| | 371/4258 [00:00<00:05, 713.63ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  12%| | 499/4258 [00:00<00:05, 702.83ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  12%| | 493/4259 [00:00<00:06, 621.10ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  14%|▏| 594/4258 [00:00<00:04, 879.51ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  10%| | 417/4258 [00:00<00:06, 611.97ex/s\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:   5%| | 229/4258 [00:00<00:11, 360.67ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  15%|▏| 629/4258 [00:00<00:04, 822.51ex/s\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  13%|▏| 544/4258 [00:00<00:04, 779.93ex/s\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:   9%| | 394/4258 [00:00<00:05, 704.32ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:   5%| | 227/4258 [00:00<00:09, 415.75ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  10%| | 447/4259 [00:00<00:07, 494.77ex/s\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  10%| | 445/4258 [00:00<00:05, 648.00ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  13%|▏| 562/4259 [00:00<00:05, 639.62ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  16%|▏| 683/4258 [00:00<00:04, 875.53ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  14%|▏| 575/4258 [00:00<00:05, 623.16ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  11%| | 480/4258 [00:00<00:06, 578.62ex/s\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  11%| | 475/4258 [00:00<00:05, 736.66ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:   6%| | 270/4258 [00:00<00:09, 417.10ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:   6%| | 266/4258 [00:00<00:12, 330.76ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  15%|▏| 623/4258 [00:00<00:05, 718.21ex/s\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  17%|▏| 714/4258 [00:00<00:04, 730.29ex/s\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  12%| | 502/4259 [00:00<00:07, 508.39ex/s\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  15%|▏| 645/4259 [00:00<00:05, 691.68ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:   8%| | 343/4258 [00:00<00:08, 456.55ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  15%|▏| 657/4258 [00:00<00:05, 670.12ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  13%|▏| 543/4258 [00:00<00:06, 592.57ex/s\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  18%|▏| 772/4258 [00:00<00:04, 806.66ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:   7%| | 319/4258 [00:00<00:10, 387.23ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  17%|▏| 743/4258 [00:00<00:04, 856.48ex/s\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  13%|▏| 551/4258 [00:00<00:05, 672.89ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  13%|▏| 555/4259 [00:01<00:07, 510.88ex/s\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  19%|▏| 791/4258 [00:01<00:04, 714.71ex/s\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  14%|▏| 594/4258 [00:00<00:05, 679.72ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:   7%| | 313/4258 [00:00<00:10, 360.96ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  14%|▏| 616/4258 [00:01<00:05, 631.65ex/s\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  17%|▏| 717/4259 [00:01<00:05, 636.21ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  20%|▏| 854/4258 [00:01<00:04, 805.81ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:   9%| | 373/4258 [00:00<00:09, 428.01ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  20%|▏| 835/4258 [00:01<00:03, 872.00ex/s\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:   9%| | 390/4258 [00:00<00:09, 395.77ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  15%|▏| 621/4258 [00:00<00:05, 641.60ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  20%|▏| 865/4258 [00:01<00:04, 693.17ex/s\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  16%|▏| 664/4258 [00:01<00:05, 619.65ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  16%|▏| 684/4258 [00:01<00:05, 645.70ex/s\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  18%|▏| 785/4259 [00:01<00:05, 647.53ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  22%|▏| 936/4258 [00:01<00:04, 792.52ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  10%| | 423/4258 [00:01<00:08, 446.97ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  19%|▏| 797/4258 [00:01<00:05, 620.91ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  14%|▏| 607/4259 [00:01<00:09, 400.91ex/s\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  16%|▏| 687/4258 [00:01<00:05, 636.31ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  22%|▏| 939/4258 [00:01<00:04, 703.02ex/s\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  18%|▏| 766/4258 [00:01<00:05, 696.18ex/s\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:   9%| | 401/4258 [00:01<00:10, 377.76ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  20%|▏| 863/4259 [00:01<00:04, 684.35ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  22%|▏| 924/4258 [00:01<00:04, 706.57ex/s\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  11%| | 475/4258 [00:01<00:08, 468.06ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  10%| | 432/4258 [00:01<00:12, 308.73ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  20%|▏| 861/4258 [00:01<00:05, 601.90ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  18%|▏| 762/4258 [00:01<00:05, 666.68ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  24%|▏| 1011/4258 [00:01<00:04, 693.59ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  24%|▏| 1016/4258 [00:01<00:04, 665.60ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  15%|▏| 651/4259 [00:01<00:09, 370.80ex/s\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  19%|▏| 793/4258 [00:01<00:05, 620.88ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  22%|▏| 939/4259 [00:01<00:04, 702.27ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  20%|▏| 837/4258 [00:01<00:05, 678.03ex/s\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  22%|▏| 926/4258 [00:01<00:05, 614.17ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  11%| | 472/4258 [00:01<00:11, 328.92ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  12%| | 523/4258 [00:01<00:08, 439.33ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  20%|▏| 833/4258 [00:01<00:05, 678.21ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  24%|▏| 1001/4258 [00:01<00:05, 614.06ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  25%|▎| 1082/4258 [00:01<00:04, 665.82ex/\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  16%|▏| 691/4259 [00:01<00:09, 369.62ex/s\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  21%|▏| 879/4258 [00:01<00:04, 687.19ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  24%|▏| 1021/4259 [00:01<00:04, 735.55ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  23%|▏| 964/4258 [00:01<00:03, 848.51ex/s\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  26%|▎| 1086/4258 [00:01<00:05, 628.10ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  12%| | 525/4258 [00:01<00:09, 375.98ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  23%|▏| 1000/4258 [00:01<00:05, 629.67ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  22%|▏| 927/4258 [00:01<00:04, 753.32ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  13%|▏| 568/4258 [00:01<00:09, 403.55ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  25%|▎| 1068/4258 [00:01<00:05, 595.67ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  26%|▎| 1109/4259 [00:01<00:04, 776.68ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  27%|▎| 1153/4258 [00:01<00:04, 638.67ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  25%|▏| 1050/4258 [00:01<00:03, 832.20ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  17%|▏| 730/4259 [00:01<00:09, 358.46ex/s\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  27%|▎| 1150/4258 [00:01<00:05, 620.37ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  14%|▏| 576/4258 [00:01<00:08, 409.15ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  22%|▏| 949/4258 [00:01<00:05, 607.51ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  25%|▏| 1064/4258 [00:01<00:05, 616.54ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  24%|▏| 1004/4258 [00:01<00:04, 706.57ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  29%|▎| 1219/4259 [00:01<00:03, 870.61ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  29%|▎| 1219/4258 [00:01<00:04, 636.57ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  18%|▏| 767/4259 [00:01<00:09, 360.13ex/s\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  14%|▏| 610/4258 [00:01<00:10, 363.97ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  29%|▎| 1225/4258 [00:01<00:04, 651.96ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  27%|▎| 1134/4258 [00:01<00:04, 763.32ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  24%|▏| 1012/4258 [00:01<00:05, 601.83ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  27%|▎| 1138/4258 [00:01<00:04, 650.34ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  15%|▏| 620/4258 [00:01<00:09, 389.51ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  13%|▏| 573/4258 [00:01<00:11, 322.04ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  31%|▎| 1323/4259 [00:01<00:03, 917.79ex/\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  19%|▏| 804/4259 [00:01<00:09, 360.82ex/s\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  25%|▎| 1076/4258 [00:01<00:04, 652.55ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  30%|▎| 1284/4258 [00:01<00:04, 623.14ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  28%|▎| 1212/4258 [00:01<00:04, 753.78ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  29%|▎| 1224/4258 [00:01<00:04, 708.95ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  25%|▎| 1074/4258 [00:01<00:05, 592.54ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  30%|▎| 1292/4258 [00:01<00:05, 587.98ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  15%|▏| 648/4258 [00:01<00:11, 323.64ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  16%|▏| 669/4258 [00:01<00:08, 409.37ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  31%|▎| 1311/4258 [00:01<00:03, 739.94ex/\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  20%|▏| 841/4259 [00:01<00:09, 362.18ex/s\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  27%|▎| 1149/4258 [00:01<00:04, 673.19ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  32%|▎| 1368/4258 [00:01<00:04, 680.24ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  14%|▏| 609/4258 [00:01<00:12, 301.20ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  31%|▎| 1306/4258 [00:01<00:03, 739.50ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  27%|▎| 1156/4258 [00:01<00:04, 653.21ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  32%|▎| 1359/4258 [00:01<00:04, 607.98ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  33%|▎| 1416/4259 [00:01<00:03, 763.67ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  30%|▎| 1289/4258 [00:01<00:04, 688.61ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  33%|▎| 1389/4258 [00:01<00:03, 750.34ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  17%|▏| 712/4258 [00:01<00:09, 370.40ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  21%|▏| 892/4259 [00:01<00:08, 402.52ex/s\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  29%|▎| 1227/4258 [00:01<00:04, 701.79ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  34%|▎| 1438/4258 [00:01<00:04, 647.41ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  15%|▏| 642/4258 [00:01<00:11, 303.59ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  33%|▎| 1402/4258 [00:01<00:03, 803.25ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  29%|▎| 1231/4258 [00:01<00:04, 679.48ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  33%|▎| 1422/4258 [00:02<00:04, 599.71ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  17%|▏| 725/4258 [00:01<00:10, 341.09ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  33%|▎| 1384/4258 [00:02<00:03, 757.48ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  18%|▏| 767/4258 [00:01<00:08, 416.12ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  34%|▎| 1467/4258 [00:02<00:03, 718.99ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  31%|▎| 1299/4258 [00:01<00:04, 701.29ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #0:  35%|▎| 1498/4259 [00:02<00:04, 681.68ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  35%|▎| 1511/4258 [00:02<00:04, 669.80ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  16%|▏| 679/4258 [00:01<00:11, 319.72ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  35%|▎| 1506/4258 [00:02<00:03, 871.09ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  31%|▎| 1306/4258 [00:02<00:04, 698.67ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  35%|▎| 1485/4258 [00:02<00:04, 607.49ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  34%|▎| 1466/4258 [00:02<00:03, 774.11ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  18%|▏| 761/4258 [00:02<00:10, 329.29ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  37%|▎| 1593/4258 [00:02<00:03, 868.15ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  19%|▏| 827/4258 [00:02<00:07, 463.43ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #0:  37%|▎| 1571/4259 [00:02<00:03, 675.29ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  17%|▏| 717/4258 [00:02<00:10, 334.88ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  32%|▎| 1370/4258 [00:02<00:04, 637.30ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  38%|▍| 1633/4258 [00:02<00:02, 988.14ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  37%|▎| 1559/4258 [00:02<00:04, 644.19ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  32%|▎| 1377/4258 [00:02<00:04, 660.09ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  37%|▎| 1579/4258 [00:02<00:04, 587.74ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  40%|▍| 1697/4258 [00:02<00:02, 916.33ex/\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  24%|▏| 1025/4259 [00:02<00:07, 428.00ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  19%|▏| 795/4258 [00:02<00:11, 308.34ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  36%|▎| 1545/4258 [00:02<00:03, 694.43ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  21%|▏| 876/4258 [00:02<00:07, 453.54ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  39%|▍| 1642/4259 [00:02<00:03, 663.35ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  34%|▎| 1456/4258 [00:02<00:04, 697.14ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  39%|▍| 1641/4258 [00:02<00:03, 690.68ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  41%|▍| 1733/4258 [00:02<00:02, 908.15ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  39%|▍| 1644/4258 [00:02<00:04, 601.58ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  34%|▎| 1444/4258 [00:02<00:04, 635.50ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  25%|▎| 1073/4259 [00:02<00:07, 440.57ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  19%|▏| 829/4258 [00:02<00:10, 314.49ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  42%|▍| 1791/4258 [00:02<00:02, 874.17ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  40%|▍| 1723/4259 [00:02<00:03, 699.90ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  22%|▏| 923/4258 [00:02<00:07, 432.74ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  36%|▎| 1540/4258 [00:02<00:03, 732.48ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  40%|▍| 1711/4258 [00:02<00:03, 661.93ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  35%|▎| 1509/4258 [00:02<00:04, 637.78ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  40%|▍| 1706/4258 [00:02<00:04, 588.25ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  38%|▍| 1617/4258 [00:02<00:04, 567.72ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  20%|▏| 871/4258 [00:02<00:09, 340.79ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  43%|▍| 1826/4258 [00:02<00:03, 803.91ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  42%|▍| 1798/4259 [00:02<00:03, 710.29ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  26%|▎| 1118/4259 [00:02<00:07, 408.45ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  38%|▍| 1627/4258 [00:02<00:03, 769.28ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  43%|▍| 1825/4258 [00:02<00:03, 795.63ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  37%|▎| 1583/4258 [00:02<00:04, 663.54ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  41%|▍| 1767/4258 [00:02<00:04, 592.61ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  44%|▍| 1881/4258 [00:02<00:03, 727.08ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  40%|▍| 1689/4258 [00:02<00:04, 601.25ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  23%|▏| 968/4258 [00:02<00:09, 364.49ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  21%|▏| 906/4258 [00:02<00:09, 338.69ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  44%|▍| 1871/4259 [00:02<00:03, 709.11ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  28%|▎| 1185/4259 [00:02<00:06, 479.12ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  45%|▍| 1910/4258 [00:02<00:03, 765.79ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  45%|▍| 1926/4258 [00:02<00:02, 855.93ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  40%|▍| 1706/4258 [00:02<00:03, 693.36ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  39%|▍| 1667/4258 [00:02<00:03, 711.02ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  43%|▍| 1843/4258 [00:02<00:03, 638.99ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  41%|▍| 1764/4258 [00:02<00:03, 636.43ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  46%|▍| 1959/4258 [00:02<00:03, 706.44ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  46%|▍| 1954/4259 [00:02<00:03, 743.18ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  22%|▏| 934/4258 [00:02<00:08, 404.42ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  29%|▎| 1235/4259 [00:02<00:06, 484.55ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  47%|▍| 2000/4258 [00:02<00:02, 792.88ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  42%|▍| 1779/4258 [00:02<00:03, 702.10ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  45%|▍| 1917/4258 [00:02<00:03, 666.83ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  43%|▍| 1835/4258 [00:02<00:03, 655.51ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  47%|▍| 2013/4258 [00:02<00:02, 786.08ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  24%|▏| 1007/4258 [00:02<00:10, 297.76ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  23%|▏| 991/4258 [00:02<00:08, 376.61ex/s\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  48%|▍| 2034/4258 [00:02<00:03, 687.22ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  41%|▍| 1739/4258 [00:02<00:04, 626.04ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  48%|▍| 2044/4259 [00:02<00:02, 784.81ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  49%|▍| 2089/4258 [00:02<00:02, 818.65ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  30%|▎| 1285/4259 [00:02<00:06, 475.68ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  47%|▍| 2014/4258 [00:02<00:02, 753.35ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  45%|▍| 1916/4258 [00:02<00:03, 696.84ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  25%|▏| 1057/4258 [00:02<00:09, 341.15ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  43%|▍| 1851/4258 [00:02<00:03, 646.82ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  24%|▏| 1032/4258 [00:02<00:08, 383.78ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  51%|▌| 2192/4259 [00:02<00:02, 986.08ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  24%|▏| 1016/4258 [00:02<00:08, 404.25ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  49%|▍| 2106/4258 [00:02<00:03, 673.39ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  49%|▍| 2094/4258 [00:02<00:03, 705.83ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  49%|▍| 2091/4258 [00:02<00:02, 726.35ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  31%|▎| 1334/4259 [00:03<00:07, 409.76ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  26%|▎| 1107/4258 [00:02<00:08, 377.14ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  45%|▍| 1918/4258 [00:02<00:03, 645.73ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  25%|▎| 1073/4258 [00:02<00:08, 390.26ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  51%|▌| 2173/4258 [00:02<00:03, 672.96ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  54%|▌| 2292/4259 [00:03<00:02, 964.97ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  51%|▌| 2170/4258 [00:03<00:02, 718.32ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  44%|▍| 1885/4258 [00:02<00:03, 626.95ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  25%|▏| 1057/4258 [00:02<00:08, 379.92ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  47%|▍| 1989/4258 [00:03<00:03, 594.21ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  32%|▎| 1377/4259 [00:03<00:07, 406.91ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  27%|▎| 1149/4258 [00:02<00:08, 379.65ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  47%|▍| 1984/4258 [00:02<00:03, 617.74ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  53%|▌| 2246/4258 [00:03<00:03, 657.94ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  26%|▎| 1113/4258 [00:03<00:08, 365.61ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  54%|▌| 2282/4258 [00:03<00:02, 736.92ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  46%|▍| 1966/4258 [00:03<00:03, 675.72ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  26%|▎| 1122/4258 [00:02<00:06, 454.75ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  48%|▍| 2053/4258 [00:03<00:03, 593.08ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  53%|▌| 2244/4258 [00:03<00:02, 674.36ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  51%|▌| 2165/4258 [00:03<00:03, 596.09ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  28%|▎| 1195/4258 [00:03<00:07, 400.33ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  56%|▌| 2390/4259 [00:03<00:02, 763.87ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  28%|▎| 1175/4258 [00:03<00:07, 432.21ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  33%|▎| 1419/4259 [00:03<00:07, 372.42ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  48%|▍| 2036/4258 [00:03<00:03, 675.96ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  28%|▎| 1173/4258 [00:03<00:06, 468.34ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  54%|▌| 2316/4258 [00:03<00:03, 623.86ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  55%|▌| 2325/4258 [00:03<00:02, 709.29ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  50%|▍| 2118/4258 [00:03<00:03, 605.74ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  55%|▌| 2358/4258 [00:03<00:02, 649.23ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  52%|▌| 2229/4258 [00:03<00:03, 568.13ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  50%|▌| 2138/4258 [00:03<00:03, 691.87ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  29%|▎| 1238/4258 [00:03<00:07, 386.56ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  58%|▌| 2474/4259 [00:03<00:02, 735.94ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  34%|▎| 1458/4259 [00:03<00:07, 362.59ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  29%|▎| 1221/4258 [00:03<00:06, 460.20ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  56%|▌| 2398/4258 [00:03<00:02, 670.27ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  56%|▌| 2398/4258 [00:03<00:02, 702.71ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  51%|▌| 2181/4258 [00:03<00:03, 584.82ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  49%|▍| 2105/4258 [00:03<00:03, 604.84ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  54%|▌| 2298/4258 [00:03<00:03, 595.24ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  57%|▌| 2426/4258 [00:03<00:02, 641.58ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  52%|▌| 2223/4258 [00:03<00:02, 735.35ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  30%|▎| 1279/4258 [00:03<00:07, 374.98ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  60%|▌| 2553/4259 [00:03<00:02, 740.22ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  58%|▌| 2478/4258 [00:03<00:02, 699.59ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  35%|▎| 1496/4259 [00:03<00:07, 351.21ex/\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  58%|▌| 2470/4258 [00:03<00:02, 678.17ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  53%|▌| 2242/4258 [00:03<00:03, 580.41ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  56%|▌| 2376/4258 [00:03<00:02, 643.11ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  30%|▎| 1268/4258 [00:03<00:07, 416.33ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  59%|▌| 2499/4258 [00:03<00:02, 662.64ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  51%|▌| 2168/4258 [00:03<00:03, 581.12ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  54%|▌| 2304/4258 [00:03<00:02, 755.46ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  32%|▎| 1344/4258 [00:03<00:06, 447.73ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  32%|▎| 1351/4258 [00:03<00:05, 502.65ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  62%|▌| 2631/4259 [00:03<00:02, 728.52ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  36%|▎| 1541/4259 [00:03<00:07, 376.24ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  55%|▌| 2322/4258 [00:03<00:03, 639.19ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  57%|▌| 2443/4258 [00:03<00:02, 638.79ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  53%|▌| 2248/4258 [00:03<00:03, 636.07ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  60%|▌| 2539/4258 [00:03<00:02, 635.20ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  60%|▌| 2567/4258 [00:03<00:02, 633.11ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  56%|▌| 2398/4258 [00:03<00:02, 808.80ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  64%|▋| 2707/4259 [00:03<00:02, 732.72ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  37%|▎| 1580/4259 [00:03<00:07, 378.22ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  31%|▎| 1311/4258 [00:03<00:08, 343.86ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  33%|▎| 1402/4258 [00:03<00:06, 460.58ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  59%|▌| 2513/4258 [00:03<00:02, 655.09ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  54%|▌| 2319/4258 [00:03<00:02, 656.03ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  62%|▌| 2650/4258 [00:03<00:02, 667.33ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  56%|▌| 2388/4258 [00:03<00:03, 593.84ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  61%|▌| 2604/4258 [00:03<00:02, 622.99ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  62%|▌| 2634/4258 [00:03<00:02, 641.71ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  58%|▌| 2480/4258 [00:03<00:02, 788.01ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  66%|▋| 2815/4259 [00:03<00:01, 827.32ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  38%|▍| 1625/4259 [00:03<00:06, 396.00ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  32%|▎| 1351/4258 [00:03<00:08, 356.21ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  61%|▌| 2586/4258 [00:03<00:02, 672.85ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  56%|▌| 2404/4258 [00:03<00:02, 707.78ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  65%|▋| 2749/4258 [00:03<00:02, 749.36ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  58%|▌| 2457/4258 [00:03<00:02, 619.78ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  63%|▋| 2685/4258 [00:03<00:02, 671.72ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  63%|▋| 2700/4258 [00:03<00:02, 640.66ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  61%|▌| 2598/4258 [00:03<00:01, 899.80ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  68%|▋| 2900/4259 [00:03<00:01, 820.44ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  39%|▍| 1669/4259 [00:03<00:06, 404.54ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  33%|▎| 1389/4258 [00:03<00:08, 358.54ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  35%|▎| 1491/4258 [00:03<00:06, 409.08ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  63%|▋| 2662/4258 [00:03<00:02, 697.41ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  59%|▌| 2521/4258 [00:03<00:02, 613.56ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  65%|▋| 2754/4258 [00:03<00:02, 661.64ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  66%|▋| 2828/4258 [00:03<00:02, 701.01ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  65%|▋| 2765/4258 [00:03<00:02, 597.15ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  63%|▋| 2689/4258 [00:03<00:01, 855.49ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  58%|▌| 2477/4258 [00:03<00:02, 623.91ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  35%|▎| 1490/4258 [00:03<00:07, 384.30ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  40%|▍| 1710/4259 [00:04<00:06, 399.98ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  34%|▎| 1427/4258 [00:03<00:08, 347.64ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  64%|▋| 2733/4258 [00:03<00:02, 696.37ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  70%|▋| 2984/4259 [00:04<00:01, 713.26ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  67%|▋| 2840/4258 [00:04<00:01, 712.59ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  60%|▌| 2551/4258 [00:03<00:02, 653.59ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  66%|▋| 2826/4258 [00:04<00:02, 583.57ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  36%|▎| 1537/4258 [00:03<00:06, 401.17ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  68%|▋| 2902/4258 [00:04<00:02, 662.10ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  41%|▍| 1751/4259 [00:04<00:06, 401.31ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  36%|▎| 1534/4258 [00:03<00:08, 325.06ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  67%|▋| 2850/4258 [00:04<00:01, 832.87ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  34%|▎| 1463/4258 [00:03<00:08, 343.89ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  65%|▋| 2776/4258 [00:03<00:02, 722.39ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  63%|▋| 2682/4258 [00:04<00:02, 714.96ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #0:  72%|▋| 3059/4259 [00:04<00:01, 657.14ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  62%|▌| 2619/4258 [00:04<00:02, 643.45ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  68%|▋| 2885/4258 [00:04<00:02, 571.79ex/\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  42%|▍| 1798/4259 [00:04<00:05, 418.86ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  70%|▋| 2971/4258 [00:04<00:01, 646.03ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  69%|▋| 2942/4258 [00:04<00:01, 857.17ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  37%|▎| 1579/4258 [00:04<00:07, 369.67ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  37%|▎| 1570/4258 [00:04<00:08, 325.18ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  67%|▋| 2858/4258 [00:04<00:01, 744.35ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  71%|▋| 3012/4258 [00:04<00:01, 779.11ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  35%|▎| 1499/4258 [00:04<00:08, 314.47ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  65%|▋| 2755/4258 [00:04<00:02, 668.77ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  69%|▋| 2949/4258 [00:04<00:02, 590.48ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  73%|▋| 3128/4259 [00:04<00:01, 635.56ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  44%|▍| 1853/4259 [00:04<00:05, 454.45ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  71%|▋| 3038/4258 [00:04<00:01, 633.72ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  38%|▍| 1626/4258 [00:04<00:06, 381.39ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  73%|▋| 3091/4258 [00:04<00:01, 775.63ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  69%|▋| 2936/4258 [00:04<00:01, 720.94ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  38%|▍| 1618/4258 [00:04<00:07, 346.57ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  36%|▎| 1532/4258 [00:04<00:08, 317.44ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  71%|▋| 3015/4258 [00:04<00:02, 607.32ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  75%|▋| 3194/4259 [00:04<00:01, 638.65ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  73%|▋| 3111/4258 [00:04<00:01, 659.21ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  65%|▋| 2783/4258 [00:04<00:02, 641.19ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  66%|▋| 2824/4258 [00:04<00:02, 575.34ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  45%|▍| 1899/4259 [00:04<00:05, 420.90ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  39%|▍| 1668/4258 [00:04<00:06, 371.15ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  74%|▋| 3169/4258 [00:04<00:01, 757.33ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  39%|▍| 1669/4258 [00:04<00:06, 386.46ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  37%|▎| 1573/4258 [00:04<00:07, 341.56ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  71%|▋| 3011/4258 [00:04<00:01, 694.14ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  72%|▋| 3087/4258 [00:04<00:01, 637.23ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  77%|▊| 3277/4259 [00:04<00:01, 683.56ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  67%|▋| 2852/4258 [00:04<00:02, 653.27ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  68%|▋| 2899/4258 [00:04<00:02, 618.93ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  46%|▍| 1978/4259 [00:04<00:04, 521.75ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  75%|▋| 3178/4258 [00:04<00:01, 615.62ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  40%|▍| 1710/4258 [00:04<00:06, 382.51ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  73%|▋| 3101/4258 [00:04<00:01, 748.83ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  38%|▍| 1608/4258 [00:04<00:07, 337.36ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  76%|▊| 3246/4258 [00:04<00:01, 727.18ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  40%|▍| 1708/4258 [00:04<00:07, 338.22ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  74%|▋| 3152/4258 [00:04<00:01, 622.96ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  79%|▊| 3347/4259 [00:04<00:01, 673.25ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  69%|▋| 2920/4258 [00:04<00:02, 654.24ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  48%|▍| 2037/4259 [00:04<00:04, 539.26ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  76%|▊| 3250/4258 [00:04<00:01, 643.05ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  39%|▍| 1653/4258 [00:04<00:07, 367.03ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  78%|▊| 3320/4258 [00:04<00:01, 725.12ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  75%|▋| 3178/4258 [00:04<00:01, 737.56ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  70%|▋| 2964/4258 [00:04<00:02, 546.42ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  41%|▍| 1750/4258 [00:04<00:07, 356.21ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  81%|▊| 3433/4259 [00:04<00:01, 724.26ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  76%|▊| 3253/4258 [00:04<00:01, 696.28ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  70%|▋| 3000/4258 [00:04<00:01, 688.36ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  49%|▍| 2093/4259 [00:04<00:03, 542.70ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  41%|▍| 1750/4258 [00:04<00:07, 324.08ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  78%|▊| 3327/4258 [00:04<00:01, 675.97ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  80%|▊| 3406/4258 [00:04<00:01, 759.64ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  40%|▍| 1691/4258 [00:04<00:07, 352.81ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  71%|▋| 3022/4258 [00:04<00:02, 542.39ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  42%|▍| 1788/4258 [00:04<00:06, 361.77ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  83%|▊| 3546/4259 [00:04<00:00, 837.69ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  78%|▊| 3329/4258 [00:04<00:01, 711.71ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  72%|▋| 3084/4258 [00:04<00:01, 731.03ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  42%|▍| 1792/4258 [00:04<00:07, 346.77ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  76%|▊| 3253/4258 [00:04<00:01, 630.72ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  50%|▌| 2148/4259 [00:04<00:04, 502.83ex/\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  82%|▊| 3490/4258 [00:04<00:00, 781.09ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  43%|▍| 1848/4258 [00:04<00:05, 424.59ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  80%|▊| 3412/4258 [00:04<00:01, 795.50ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  72%|▋| 3079/4258 [00:04<00:02, 539.96ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  85%|▊| 3633/4259 [00:04<00:00, 846.72ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  80%|▊| 3402/4258 [00:04<00:01, 716.45ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  74%|▋| 3164/4258 [00:04<00:01, 748.84ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  43%|▍| 1830/4258 [00:04<00:06, 354.47ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  79%|▊| 3346/4258 [00:04<00:01, 704.65ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  41%|▍| 1727/4258 [00:04<00:08, 309.31ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  82%|▊| 3501/4258 [00:04<00:00, 819.52ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  44%|▍| 1892/4258 [00:04<00:05, 423.18ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  74%|▋| 3139/4258 [00:04<00:02, 553.49ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  84%|▊| 3569/4258 [00:04<00:00, 727.42ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  81%|▊| 3461/4258 [00:04<00:01, 596.25ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  52%|▌| 2200/4259 [00:05<00:04, 434.95ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  82%|▊| 3475/4258 [00:04<00:01, 677.50ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  44%|▍| 1868/4258 [00:04<00:06, 360.87ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  87%|▊| 3719/4259 [00:05<00:00, 727.71ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  76%|▊| 3240/4258 [00:04<00:01, 647.91ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  84%|▊| 3597/4258 [00:05<00:00, 857.41ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  86%|▊| 3650/4258 [00:05<00:00, 749.32ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  80%|▊| 3421/4258 [00:04<00:01, 613.57ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  75%|▊| 3196/4258 [00:05<00:01, 551.83ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  83%|▊| 3523/4258 [00:05<00:01, 591.08ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  45%|▍| 1936/4258 [00:04<00:05, 393.01ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  45%|▍| 1909/4258 [00:05<00:06, 369.38ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  83%|▊| 3544/4258 [00:05<00:01, 658.84ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  42%|▍| 1806/4258 [00:04<00:07, 333.94ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  53%|▌| 2246/4259 [00:05<00:05, 374.39ex/\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  87%|▊| 3691/4258 [00:05<00:00, 879.11ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  88%|▉| 3765/4258 [00:05<00:00, 862.02ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  89%|▉| 3796/4259 [00:05<00:00, 671.49ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  85%|▊| 3606/4258 [00:05<00:00, 655.03ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  78%|▊| 3308/4258 [00:05<00:01, 619.13ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  76%|▊| 3253/4258 [00:05<00:01, 543.45ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  85%|▊| 3616/4258 [00:05<00:00, 672.51ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  43%|▍| 1845/4258 [00:05<00:06, 347.28ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  46%|▍| 1947/4258 [00:05<00:06, 332.40ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  84%|▊| 3565/4258 [00:05<00:01, 659.09ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  86%|▊| 3676/4258 [00:05<00:00, 664.04ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  54%|▌| 2286/4259 [00:05<00:05, 362.86ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  78%|▊| 3332/4258 [00:05<00:01, 611.14ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  46%|▍| 1977/4258 [00:05<00:07, 323.82ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  90%|▉| 3853/4258 [00:05<00:00, 823.96ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  79%|▊| 3372/4258 [00:05<00:01, 594.06ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  87%|▊| 3684/4258 [00:05<00:00, 673.58ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  44%|▍| 1881/4258 [00:05<00:06, 346.97ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  91%|▉| 3867/4259 [00:05<00:00, 568.08ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  86%|▊| 3649/4258 [00:05<00:00, 705.84ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  88%|▉| 3759/4258 [00:05<00:00, 709.75ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  80%|▊| 3409/4258 [00:05<00:01, 656.09ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  47%|▍| 2019/4258 [00:05<00:06, 346.04ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  55%|▌| 2324/4259 [00:05<00:05, 361.95ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  47%|▍| 1982/4258 [00:05<00:07, 300.88ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  81%|▊| 3435/4258 [00:05<00:01, 597.30ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  88%|▉| 3756/4258 [00:05<00:00, 684.86ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  45%|▍| 1919/4258 [00:05<00:06, 355.04ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  92%|▉| 3929/4259 [00:05<00:00, 566.55ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  92%|▉| 3937/4258 [00:05<00:00, 661.57ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  48%|▍| 2058/4258 [00:05<00:06, 356.97ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  82%|▊| 3476/4258 [00:05<00:01, 642.43ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  55%|▌| 2362/4259 [00:05<00:05, 361.12ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  83%|▊| 3515/4258 [00:05<00:01, 651.13ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  47%|▍| 2014/4258 [00:05<00:07, 300.43ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  87%|▊| 3723/4258 [00:05<00:00, 646.72ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  90%|▉| 3826/4258 [00:05<00:00, 688.52ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  46%|▍| 1966/4258 [00:05<00:05, 387.38ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  90%|▉| 3832/4258 [00:05<00:00, 581.48ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  94%|▉| 3999/4259 [00:05<00:00, 593.39ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  49%|▍| 2107/4258 [00:05<00:05, 392.07ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  84%|▊| 3585/4258 [00:05<00:00, 770.28ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  56%|▌| 2402/4259 [00:05<00:05, 369.64ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  85%|▊| 3610/4258 [00:05<00:00, 733.17ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  48%|▍| 2049/4258 [00:05<00:07, 311.73ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  94%|▉| 4009/4258 [00:05<00:00, 624.18ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  89%|▉| 3791/4258 [00:05<00:00, 640.70ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  92%|▉| 3914/4258 [00:05<00:00, 742.28ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  47%|▍| 2006/4258 [00:05<00:06, 363.01ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  96%|▉| 4080/4259 [00:05<00:00, 647.02ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  91%|▉| 3895/4258 [00:05<00:00, 559.52ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  51%|▌| 2152/4258 [00:05<00:05, 406.50ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  86%|▊| 3683/4258 [00:05<00:00, 830.47ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  57%|▌| 2441/4259 [00:05<00:04, 374.38ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  87%|▊| 3688/4258 [00:05<00:00, 744.87ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  49%|▍| 2087/4258 [00:05<00:06, 329.61ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  96%|▉| 4098/4258 [00:05<00:00, 683.75ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  94%|▉| 3991/4258 [00:05<00:00, 749.86ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  91%|▉| 3864/4258 [00:05<00:00, 660.59ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  98%|▉| 4154/4259 [00:05<00:00, 670.59ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  96%|▉| 4081/4258 [00:05<00:00, 696.33ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  93%|▉| 3961/4258 [00:05<00:00, 583.21ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  52%|▌| 2195/4258 [00:05<00:05, 406.91ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  58%|▌| 2482/4259 [00:05<00:04, 384.03ex/\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2:  98%|▉| 4174/4258 [00:05<00:00, 702.31ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  50%|▌| 2135/4258 [00:05<00:05, 365.54ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  96%|▉| 4107/4258 [00:05<00:00, 870.34ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  92%|▉| 3932/4258 [00:05<00:00, 665.91ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  88%|▉| 3767/4258 [00:05<00:00, 750.33ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  88%|▉| 3764/4258 [00:05<00:00, 654.59ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #0:  99%|▉| 4229/4259 [00:05<00:00, 691.96ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  94%|▉| 4022/4258 [00:05<00:00, 585.67ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  53%|▌| 2246/4258 [00:05<00:04, 434.44ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #0: 100%|█| 4259/4259 [00:05<00:00, 715.19ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  98%|▉| 4152/4258 [00:05<00:00, 651.76ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  51%|▌| 2175/4258 [00:05<00:05, 375.18ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  94%|▉| 4006/4258 [00:05<00:00, 683.32ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6:  99%|▉| 4195/4258 [00:05<00:00, 859.59ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  90%|▉| 3845/4258 [00:05<00:00, 738.71ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running tokenizer on train dataset #2: 100%|▉| 4248/4258 [00:05<00:00, 643.32ex/\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #2: 100%|█| 4258/4258 [00:06<00:00, 708.77ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  96%|▉| 4089/4258 [00:05<00:00, 608.43ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  50%|▌| 2140/4258 [00:05<00:05, 374.50ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  61%|▌| 2587/4259 [00:06<00:03, 457.86ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  54%|▌| 2291/4258 [00:05<00:04, 433.93ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #3:  99%|▉| 4224/4258 [00:06<00:00, 668.46ex/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  52%|▌| 2218/4258 [00:05<00:05, 390.26ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #6: 100%|█| 4258/4258 [00:05<00:00, 709.86ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #3: 100%|█| 4258/4258 [00:06<00:00, 702.98ex/\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  93%|▉| 3941/4258 [00:05<00:00, 754.90ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  92%|▉| 3921/4258 [00:06<00:00, 672.55ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  98%|▉| 4156/4258 [00:06<00:00, 625.01ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  52%|▌| 2194/4258 [00:05<00:04, 419.31ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  62%|▌| 2645/4259 [00:06<00:03, 492.79ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  53%|▌| 2264/4258 [00:06<00:04, 409.88ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11:  98%|▉| 4164/4258 [00:05<00:00, 689.22ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  55%|▌| 2335/4258 [00:06<00:05, 373.67ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  95%|▉| 4065/4258 [00:06<00:00, 890.50ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  94%|▉| 3991/4258 [00:06<00:00, 669.10ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5:  99%|▉| 4235/4258 [00:06<00:00, 671.07ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  54%|▌| 2295/4258 [00:06<00:03, 583.95ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  64%|▋| 2735/4259 [00:06<00:02, 612.39ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #5: 100%|█| 4258/4258 [00:06<00:00, 687.25ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11: 100%|▉| 4254/4258 [00:06<00:00, 744.05ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #11: 100%|█| 4258/4258 [00:06<00:00, 699.15ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8:  98%|▉| 4157/4258 [00:06<00:00, 853.83ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  57%|▌| 2410/4258 [00:06<00:02, 745.14ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  95%|▉| 4060/4258 [00:06<00:00, 661.85ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  66%|▋| 2809/4259 [00:06<00:02, 649.44ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  57%|▌| 2407/4258 [00:06<00:03, 566.43ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  58%|▌| 2451/4258 [00:06<00:03, 474.07ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  59%|▌| 2493/4258 [00:06<00:02, 767.33ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  97%|▉| 4137/4258 [00:06<00:00, 689.89ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #8: 100%|▉| 4245/4258 [00:06<00:00, 819.15ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #8: 100%|█| 4258/4258 [00:06<00:00, 673.33ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  58%|▌| 2480/4258 [00:06<00:02, 612.61ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  59%|▌| 2503/4258 [00:06<00:03, 484.35ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4:  99%|▉| 4226/4258 [00:06<00:00, 744.33ex/\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  71%|▋| 3025/4259 [00:06<00:01, 842.70ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  60%|▌| 2564/4258 [00:06<00:02, 679.89ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #4: 100%|█| 4258/4258 [00:06<00:00, 652.30ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  60%|▌| 2567/4258 [00:06<00:03, 527.11ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  62%|▌| 2653/4258 [00:06<00:02, 742.26ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  73%|▋| 3110/4259 [00:06<00:01, 768.18ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  62%|▌| 2645/4258 [00:06<00:02, 697.84ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  62%|▌| 2641/4258 [00:06<00:02, 587.66ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  65%|▋| 2757/4258 [00:06<00:01, 828.94ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  75%|▋| 3189/4259 [00:06<00:01, 767.06ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  64%|▋| 2717/4258 [00:06<00:02, 673.28ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  64%|▋| 2712/4258 [00:06<00:02, 622.31ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  67%|▋| 2841/4258 [00:06<00:01, 806.55ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  77%|▊| 3270/4259 [00:06<00:01, 776.97ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  66%|▋| 2806/4258 [00:06<00:01, 731.24ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  66%|▋| 2800/4258 [00:06<00:02, 696.42ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  70%|▋| 2967/4258 [00:06<00:01, 932.35ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  68%|▋| 2898/4258 [00:06<00:01, 779.74ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  68%|▋| 2881/4258 [00:06<00:02, 661.50ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  79%|▊| 3349/4259 [00:07<00:01, 668.17ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  72%|▋| 3061/4258 [00:06<00:01, 898.14ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  70%|▋| 2990/4258 [00:06<00:01, 818.63ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  69%|▋| 2955/4258 [00:06<00:01, 681.45ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  80%|▊| 3425/4259 [00:07<00:01, 691.75ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  72%|▋| 3080/4258 [00:07<00:01, 841.35ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  74%|▋| 3152/4258 [00:07<00:01, 809.41ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  71%|▋| 3036/4258 [00:07<00:01, 716.66ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  82%|▊| 3497/4259 [00:07<00:01, 665.47ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  75%|▋| 3189/4258 [00:07<00:01, 915.07ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  76%|▊| 3235/4258 [00:07<00:01, 813.30ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  73%|▋| 3111/4258 [00:07<00:01, 721.61ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  85%|▊| 3608/4259 [00:07<00:00, 784.00ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  75%|▋| 3191/4258 [00:07<00:01, 741.25ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  77%|▊| 3281/4258 [00:07<00:01, 824.10ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  87%|▊| 3700/4259 [00:07<00:00, 821.70ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  78%|▊| 3318/4258 [00:07<00:01, 739.02ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  77%|▊| 3266/4258 [00:07<00:01, 731.31ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  79%|▊| 3366/4258 [00:07<00:01, 806.49ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  80%|▊| 3394/4258 [00:07<00:01, 743.39ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  89%|▉| 3785/4259 [00:07<00:00, 720.92ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  78%|▊| 3340/4258 [00:07<00:01, 698.25ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  81%|▊| 3448/4258 [00:07<00:01, 789.93ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  81%|▊| 3470/4258 [00:07<00:01, 726.58ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  91%|▉| 3861/4259 [00:07<00:00, 638.14ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  80%|▊| 3411/4258 [00:07<00:01, 681.93ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  83%|▊| 3528/4258 [00:07<00:00, 751.31ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  83%|▊| 3544/4258 [00:07<00:01, 711.04ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  82%|▊| 3480/4258 [00:07<00:01, 683.49ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  92%|▉| 3929/4259 [00:07<00:00, 629.96ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  85%|▊| 3604/4258 [00:07<00:00, 752.60ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  85%|▊| 3633/4258 [00:07<00:00, 757.98ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  83%|▊| 3549/4258 [00:07<00:01, 673.08ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  94%|▉| 3999/4259 [00:07<00:00, 647.14ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  86%|▊| 3680/4258 [00:07<00:00, 739.21ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  87%|▊| 3710/4258 [00:07<00:00, 738.67ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  85%|▊| 3622/4258 [00:07<00:00, 688.97ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  95%|▉| 4066/4259 [00:08<00:00, 650.03ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  88%|▉| 3760/4258 [00:07<00:00, 756.05ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  89%|▉| 3787/4258 [00:07<00:00, 741.67ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  87%|▊| 3696/4258 [00:07<00:00, 701.98ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  97%|▉| 4151/4259 [00:08<00:00, 704.29ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  91%|▉| 3858/4258 [00:08<00:00, 819.61ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  91%|▉| 3865/4258 [00:08<00:00, 750.56ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  88%|▉| 3767/4258 [00:08<00:00, 688.20ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #1:  99%|▉| 4223/4259 [00:08<00:00, 695.50ex/\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  93%|▉| 3951/4258 [00:08<00:00, 851.66ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #1: 100%|█| 4259/4259 [00:08<00:00, 510.04ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  91%|▉| 3863/4258 [00:08<00:00, 765.73ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  95%|▉| 4051/4258 [00:08<00:00, 893.92ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  94%|▉| 4013/4258 [00:08<00:00, 696.68ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  93%|▉| 3969/4258 [00:08<00:00, 851.66ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9:  97%|▉| 4147/4258 [00:08<00:00, 912.61ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #7:  96%|▉| 4093/4258 [00:08<00:00, 724.71ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  95%|▉| 4063/4258 [00:08<00:00, 876.45ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9: 100%|▉| 4239/4258 [00:08<00:00, 822.21ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #9: 100%|█| 4258/4258 [00:08<00:00, 502.81ex/\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running tokenizer on train dataset #7: 100%|█| 4258/4258 [00:08<00:00, 496.67ex/\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10:  98%|▉| 4152/4258 [00:08<00:00, 802.17ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running tokenizer on train dataset #10: 100%|█| 4258/4258 [00:08<00:00, 493.78ex\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Using amp half precision backend\n",
      "/home/cse/anaconda3/envs/samikhan/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1713\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 645\n",
      " 33%|█████████████▌                           | 214/645 [01:28<02:46,  2.60it/s]Configuration saved in monobert-reranker-epoch-1.0/config.json\n",
      "Model weights saved in monobert-reranker-epoch-1.0/pytorch_model.bin\n",
      "loading configuration file monobert-reranker-epoch-1.0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"monobert-reranker-epoch-1.0\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 105879\n",
      "}\n",
      "\n",
      "01/30/2024 17:17:58 - INFO - tevatron.reranker.modeling -   loading model weight from local monobert-reranker-epoch-1.0\n",
      "loading weights file monobert-reranker-epoch-1.0/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at monobert-reranker-epoch-1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Traceback (most recent call last):\n",
      "  File \"train_reranker.py\", line 201, in <module>\n",
      "    main()\n",
      "  File \"train_reranker.py\", line 194, in main\n",
      "    trainer.train()  # TODO: resume training\n",
      "  File \"/home/cse/anaconda3/envs/samikhan/lib/python3.7/site-packages/transformers/trainer.py\", line 1454, in train\n",
      "    self.control = self.callback_handler.on_epoch_end(args, self.state, self.control)\n",
      "  File \"/home/cse/anaconda3/envs/samikhan/lib/python3.7/site-packages/transformers/trainer_callback.py\", line 358, in on_epoch_end\n",
      "    return self.call_event(\"on_epoch_end\", args, state, control)\n",
      "  File \"/home/cse/anaconda3/envs/samikhan/lib/python3.7/site-packages/transformers/trainer_callback.py\", line 399, in call_event\n",
      "    **kwargs,\n",
      "  File \"/home/cse/SamiKhan/monobert/evaluation_callback.py\", line 75, in on_epoch_end\n",
      "    logger.info(f\"Epoch {state.epoch}, Dataset: {self.eval_args.dataset_name}\\n {res[-78:]}\")\n",
      "AttributeError: 'EvaluationArguments' object has no attribute 'dataset_name'\n",
      " 33%|█████████████▋                           | 215/645 [04:01<08:03,  1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "!huggingface-cli login --token=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\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0 python train_reranker.py \\\n",
    "  --output_dir reranker_mrtydi-bn \\\n",
    "  --model_name_or_path bert-base-multilingual-uncased \\\n",
    "  --dataset_name castorini/mr-tydi:bengali \\\n",
    "  --fp16 \\\n",
    "  --save_strategy no \\\n",
    "  --per_device_train_batch_size 8 \\\n",
    "  --train_n_passages 8 \\\n",
    "  --learning_rate 5e-6 \\\n",
    "  --q_max_len 64 \\\n",
    "  --p_max_len 256 \\\n",
    "  --num_train_epochs 3 \\\n",
    "  --logging_steps 500 \\\n",
    "  --overwrite_output_dir \\\n",
    "  --eval_dataset_name anwesha-news \\\n",
    "  --collection_path /media/cse/HDD/SamiKhan/anwesha/anwesha-news-collection/anwesha-news-collection.jsonl \\\n",
    "  --topics_path /media/cse/HDD/SamiKhan/anwesha/anwesha-news-topics.tsv \\\n",
    "  --qrels_path /media/cse/HDD/SamiKhan/anwesha/anwesha-news-qrels.txt \\\n",
    "  --retrieval_results /media/cse/HDD/SamiKhan/anwesha/runs/run.anwesha-news.bm25.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vq9SM70739YE"
   },
   "source": [
    "## Evaluate reranker (BM25 + MonoBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wi6Jn8MjG5rA",
    "outputId": "61851b2b-cea5-451a-eae7-215943a4e322"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 134/134 [00:00<00:00, 1724039.07it/s]\n",
      "100%|████████████████████████████████████████| 133/133 [00:01<00:00, 126.69it/s]\n"
     ]
    }
   ],
   "source": [
    "data_folder_root = \"/media/cse/HDD/SamiKhan/anwesha/\"\n",
    "model_name_or_path = \"reranker_mrtydi-bn\"\n",
    "dataset_name = \"anwesha-news\"\n",
    "\n",
    "!python reranker_eval.py \\\n",
    "    --collection_path {data_folder_root}/{dataset_name}-collection/{dataset_name}-collection.jsonl \\\n",
    "    --topics_path {data_folder_root}/{dataset_name}-topics.tsv \\\n",
    "    --qrels_path {data_folder_root}/{dataset_name}-qrels.txt \\\n",
    "    --retrieval_results {data_folder_root}/runs/run.{dataset_name}.bm25.txt \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --output_save_path run.{dataset_name}.monobert.txt \\\n",
    "    --fp16 True \\\n",
    "    --per_device_eval_batch_size 64 \\\n",
    "    --dataloader_num_workers 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "samikhan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
